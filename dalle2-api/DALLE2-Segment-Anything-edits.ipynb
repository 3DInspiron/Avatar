{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precise Inpainting with Segment Anything, DALLE-2 and Stable Diffusion\n",
    "<img src=\"../media/advanced-inpainting-small.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "\n",
    "This notebook guides you through the process of image inpainting, i.e., replacing an object in an image based on a text description. Here is a breakdown of the steps we'll be taking:\n",
    "\n",
    "1. **Source Image Selection:** Choose the image that you wish to modify through inpainting.\n",
    "\n",
    "2. **Object Identification & Segmentation:** Mark the object that you want to replace by providing a few coordinates of the region. Following this, you'll choose a segmentation mask, suggested by the 'Segment Anything Model' (SAM).\n",
    "\n",
    "3. **Inpainting:** We will employ either DALLE-2 or Stable Diffusion to inpaint the masked region with a new object that you define using a text prompt.\n",
    "\n",
    "*Important:* Stable Diffusion offers numerous options to finely tune the resulting image. We highly recommend referring to the [Text-guided Image Inpainting](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint) documentation for a more detailed understanding.\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision torchaudio\n",
    "# %pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "# %pip install opencv-python pycocotools matplotlib onnxruntime onnx\n",
    "\n",
    "# note depending on your setup, the following commands might be necessary to import cv2:\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install ffmpeg libsm6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import plotly.graph_objects as go\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from utils import show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read API key of original OpenAI service from .enf file\n",
    "load_dotenv('../.env')\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Uncomment and run once to download the SAM model\n",
    "#!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "# initialize SAM model\n",
    "sam_model_filepath = \"./sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[\"default\"](checkpoint=sam_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax):\n",
    "    \"\"\" Display segmentation mask \"\"\"\n",
    "    color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    \"\"\" Display coordinates for mask creation \"\"\"\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_img = './source/no-smile-1024.png'\n",
    "image = Image.open(source_img)\n",
    "\n",
    "height = image.height\n",
    "width = image.width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Coordinates for Mask Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to numpy array\n",
    "np_image = np.array(image)\n",
    "\n",
    "# Create a figure using Plotly\n",
    "fig = go.Figure(data=go.Image(z=np_image))\n",
    "fig.update_layout(width=width, height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter any number of x,y coordinates for the mask:   \n",
    "input_points = np.array([[515, 702], [700, 645], [275, 1005], [900, 1002]])\n",
    "input_labels = np.ones(input_points.shape[0], dtype=int)\n",
    "\n",
    "# Load chosen image using opencv\n",
    "image = cv2.imread(source_img)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_points(input_points, input_labels, plt.gca())\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate predictor with Segment Anything model\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image)\n",
    "\n",
    "# Use the predictor to gather masks for the point we clicked\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_points,\n",
    "    point_labels=input_labels,\n",
    "    multimask_output=True,\n",
    ")\n",
    "\n",
    "# Check the shape - should be three masks of the same dimensions as our image\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_masks = len(masks)\n",
    "fig = plt.figure(figsize=(10*n_masks, 10))  # adjust the total width according to the number of masks\n",
    "\n",
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    ax = fig.add_subplot(1, n_masks, i+1)  # Create 1 row of n_masks columns of subplots\n",
    "    ax.imshow(image)\n",
    "    show_mask(mask, ax)\n",
    "    show_points(input_points, input_labels, ax)\n",
    "    ax.set_title(f\"Mask {i}, Score: {score:.3f}\", fontsize=18)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which mask you'd like to use (zero indexed)\n",
    "chosen_mask = masks[1]\n",
    "\n",
    "# We'll now reverse the mask so that it is clear and everything else is white\n",
    "chosen_mask = chosen_mask.astype(\"uint8\")\n",
    "chosen_mask[chosen_mask != 0] = 255\n",
    "chosen_mask[chosen_mask == 0] = 1\n",
    "chosen_mask[chosen_mask == 255] = 0\n",
    "chosen_mask[chosen_mask == 1] = 255\n",
    "\n",
    "# create a base blank mask\n",
    "mask = Image.new(\"RGBA\", (width, height), (0, 0, 0, 1))  # create an opaque image mask\n",
    "\n",
    "# Convert mask back to pixels to add our mask replacing the third dimension\n",
    "pix = np.array(mask)\n",
    "pix[:, :, 3] = chosen_mask\n",
    "\n",
    "# Convert pixels back to an RGBA image and display\n",
    "new_mask = Image.fromarray(pix, \"RGBA\")\n",
    "\n",
    "# save mask for re-use for our edit\n",
    "mask_image = './mask.png'\n",
    "new_mask.save(mask_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpaint with DALLE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Image.create_edit(\n",
    "  image=open(source_img, \"rb\"),\n",
    "  mask=open(mask_image, \"rb\"),\n",
    "  prompt=\"women polo-shirt, detailed, photorealistic, professional fashion photo shooting, fashion magazine\",\n",
    "  n=6,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "images = [image['url'] for image in response['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(images, cols=3, savedir='./results/no-smile/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpaint with Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import ImageOps\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\", # options: runwayml/stable-diffusion-inpainting, stabilityai/stable-diffusion-2-inpainting\n",
    "    revision=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_image = Image.open(source_img)\n",
    "mask_image = Image.open('./mask.png')\n",
    "inv_mask = mask_image.split()[3]\n",
    "mask = ImageOps.invert(inv_mask)\n",
    "\n",
    "prompt = \"women summer polo-shirt, detailed, photorealistic, professional fashion photo shooting, fashion magazine\"\n",
    "negative_prompt = 'deformed, disfigured, underexposed, overexposed'\n",
    "\n",
    "result = pipe(prompt=prompt,\n",
    "             negative_prompt= negative_prompt, \n",
    "             image=src_image,\n",
    "             mask_image=mask,\n",
    "             height = height,\n",
    "             width = width,\n",
    "             num_images_per_prompt=6,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = './results/sd/no-smile'\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "files = glob.glob(f'{savedir}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# Get all png files in the folder\n",
    "files = glob.glob(f'{savedir}/*.png')\n",
    "\n",
    "# Extract the numeric parts from the filenames\n",
    "numbers = [int(re.findall(r'(\\d+)', file)[0]) for file in files if re.findall(r'(\\d+)', file)]\n",
    "\n",
    "# Return the highest number, or 0 if no files exist\n",
    "idx = max(numbers) if numbers else 0\n",
    "\n",
    "images = []\n",
    "for image in result.images:\n",
    "    image_path = os.path.join(savedir, f'{idx:003}.png')\n",
    "    image.save(image_path)\n",
    "    images.append(image_path)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(images, source='local', cols=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Video from the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(image_folder, video_name, fps, blend_frames=10):\n",
    "    \"\"\"\n",
    "    Create a video from a folder of images with cross-fading effect.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: str, path to the folder containing the images\n",
    "    - video_name: str, name of the output video file (including .mp4 extension)\n",
    "    - fps: int, frames per second \n",
    "    - blend_frames: int, number of frames for blending effect\n",
    "    \"\"\"\n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "    images.sort()\n",
    "\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    for i in range(len(images) - 1):\n",
    "        image1 = cv2.imread(os.path.join(image_folder, images[i]))\n",
    "        image2 = cv2.imread(os.path.join(image_folder, images[i + 1]))\n",
    "\n",
    "        # Create a transition from image1 to image2 over blend_frames\n",
    "        for j in range(blend_frames):\n",
    "            alpha = j / blend_frames  # Calculate the weight of the second image\n",
    "            blended = cv2.addWeighted(image1, 1 - alpha, image2, alpha, 0)  # Blend the two images\n",
    "            video.write(np.uint8(blended))\n",
    "\n",
    "    # Write the last frame\n",
    "    video.write(cv2.imread(os.path.join(image_folder, images[-1])))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "# Usage\n",
    "# create_video('./results/smile', 'output.mp4', 3, blend_frames=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
